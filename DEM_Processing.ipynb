{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import richdem as rd\n",
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "from osgeo import osr, gdal\n",
    "import rasterstats as rs\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import colors\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import rasterio as rio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import plotting_extent, show\n",
    "from shapely.geometry import mapping, box, Point, Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import shapefile\n",
    "from linecache import getline\n",
    "\n",
    "import earthpy as et\n",
    "import earthpy.spatial as es\n",
    "import earthpy.clip as cl\n",
    "\n",
    "import common_functions_2 as common\n",
    "\n",
    "# Set project constants\n",
    "ftp_data_source = 'ftp.agrc.utah.gov'\n",
    "ftp_dem_directory = '/LiDAR/WasatchFront_2m/'\n",
    "ftp_file_list = ['12TVK360840',\n",
    "                 '12TVK360860',\n",
    "                 '12TVK360880',\n",
    "                 '12TVK360900',\n",
    "                 '12TVK360920',\n",
    "                 '12TVK360940',\n",
    "                 '12TVK380840',\n",
    "                 '12TVK380860',\n",
    "                 '12TVK380880',\n",
    "                 '12TVK380900',\n",
    "                 '12TVK380920',\n",
    "                 '12TVK380940',\n",
    "                 '12TVK400840',\n",
    "                 '12TVK400860',\n",
    "                 '12TVK400880',\n",
    "                 '12TVK400900',\n",
    "                 '12TVK400920',\n",
    "                 '12TVK400940',\n",
    "                 '12TVK420840',\n",
    "                 '12TVK420860',\n",
    "                 '12TVK420880',\n",
    "                 '12TVK420900',\n",
    "                 '12TVK420920',\n",
    "                 '12TVK420940',\n",
    "                 '12TVK440860',\n",
    "                 '12TVK440880',\n",
    "                 '12TVK440900',\n",
    "                 '12TVK440920',\n",
    "                 '12TVK440940',\n",
    "                 '12TVK460880',\n",
    "                 '12TVK460900',\n",
    "                 '12TVK460920',\n",
    "                 '12TVK460940',\n",
    "                 '12TVK480880',\n",
    "                 '12TVK480900',\n",
    "                 '12TVK480920',\n",
    "                 '12TVK480940',\n",
    "                 '12TVK500900',\n",
    "                 '12TVK500920',\n",
    "                 '12TVK500940',\n",
    "                 '12TVK520900',\n",
    "                 '12TVK520920',\n",
    "                 '12TVK520940']\n",
    "\n",
    "home_dir = os.path.join(et.io.HOME, 'earth-analytics')\n",
    "data_dir = os.path.join(home_dir, 'data')\n",
    "original_DEM_data_dir = os.path.join(data_dir, 'original', 'dem')\n",
    "\n",
    "modified_data_dir = os.path.join(data_dir, 'modified')\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ftp(address: str, directory: str = None) -> FTP:\n",
    "    '''\n",
    "    Initialize an ftp connection to the specified address\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    address: str\n",
    "        The ftp address that you would like to connect to.\n",
    "    directory: str\n",
    "        The directory you would like to change to on the ftp server.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    ftp: FTP object\n",
    "        The FTP object that can be used to pull files down.\n",
    "    '''\n",
    "    ftp = FTP(address)\n",
    "    ftp.login()\n",
    "    if directory:\n",
    "        ftp.cwd(directory)\n",
    "    return ftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asc_to_merged_tiff(asc_list):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    file_list = []\n",
    "\n",
    "    # Open each asc file\n",
    "    for file in asc_list:\n",
    "        dem_path = os.path.join(original_DEM_data_dir, file + \".asc\")\n",
    "        nfile = rio.open(dem_path, no_data=-9999.0)\n",
    "        file_list += [nfile]\n",
    "\n",
    "    # Merge the asc files\n",
    "    arr, out_trans = merge(file_list, res=(2.0, 2.0), nodata=-9999.0)\n",
    "\n",
    "    # Write to a new geotiff file with crs info\n",
    "    with rio.open((os.path.join(modified_data_dir, file + \".tif\")), 'w',\n",
    "                  driver=\"GTiff\", height=arr[0].shape[0], width=arr[0].shape[1],\n",
    "                  count=1, dtype=str(arr[0].dtype),\n",
    "                  # proj4 definition for NAD83 UTM zone 12N\n",
    "                  crs=\"+proj=utm +zone=12 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\",\n",
    "                  transform=out_trans,\n",
    "                  nodata=-9999.0) as dst:\n",
    "\n",
    "        dst.write(arr[0], 1)\n",
    "\n",
    "    # Mask the source images\n",
    "    with rio.open(os.path.join(modified_data_dir, file + \".tif\")) as src:\n",
    "        image = src.read(1, masked=True)\n",
    "        pre_extent = plotting_extent(src)\n",
    "\n",
    "    return arr, out_trans, pre_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zonal_stats_dataframe(shape, data, data_transform, statistic):\n",
    "    \"\"\"\n",
    "    A wrapper around zonal stats, this packages the output into a geodataframe containing the following columns:\n",
    "        avalanche_id\n",
    "        height_bucket\n",
    "        [statistic]\n",
    "        size\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape: geopandas object\n",
    "        The input shape which is the resulting union of avalanche shapes and elevation buckets.\n",
    "        The shape input must have the following columns:    \n",
    "            avalanche_id: a unique identifier/integer for each avalanche\n",
    "            height_bucket: The height bucket that this avalanche falls under\n",
    "            geometry_sq_meters: The size of the geometry of each row\n",
    "    data: ndarray\n",
    "        The values which you would like to use in this calculation\n",
    "    data_transform: rasterio.transform\n",
    "        The transform for the data array\n",
    "    statistic: string\n",
    "        The statistic you would like to perform with rasterstats.\n",
    "    Returns\n",
    "    ----------\n",
    "    results: geopandas dataframe\n",
    "        A geopandas dataframe containing the following columns:\n",
    "            height_bucket: The bucketed height in intervals as specified within the input shape\n",
    "            avalanche_id: A unique identifier to indicate what avalanche path this shape is part of\n",
    "            [statistic]: The statistic over this shape\n",
    "            size: The size of this shape\n",
    "    \"\"\"\n",
    "    results_geojson = rs.zonal_stats(shape,\n",
    "                                     data,\n",
    "                                     affine=data_transform,\n",
    "                                     stats=statistic)\n",
    "    shape[statistic] = [i[statistic] for i in results_geojson]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and unzip source DEM data from ftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DEM data\n",
    "ftp = init_ftp(ftp_data_source, ftp_dem_directory)\n",
    "\n",
    "# Determine which files are already present and don't re-download\n",
    "if os.path.isdir(original_DEM_data_dir):\n",
    "    files_already_downloaded = os.listdir(original_DEM_data_dir)\n",
    "    files_not_present = [ftp_file\n",
    "                         for ftp_file\n",
    "                         in ftp_file_list\n",
    "                         if ftp_file + '.asc' not in files_already_downloaded]\n",
    "else:\n",
    "    files_not_present = ftp_file_list\n",
    "    os.mkdir(original_DEM_data_dir)\n",
    "\n",
    "for idx, file_name in enumerate(files_not_present):\n",
    "    target_name = os.path.join(original_DEM_data_dir, file_name + '.asc')\n",
    "\n",
    "    # Download data\n",
    "    with open(target_name, \"wb\") as file:\n",
    "        print(\"Downloading %d of %d (%s)\" %\n",
    "              (idx + 1, len(files_not_present), file_name))\n",
    "        ftp.retrbinary(\"RETR \" + file_name + \".asc\", file.write)\n",
    "\n",
    "print(\"Completed download of dem data.\")\n",
    "\n",
    "# Make modified DEM directory if one doesn't exist yet.\n",
    "if os.path.exists(modified_data_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(modified_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define study area and spatial extents for cropping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open avalanche boundary\n",
    "avalanche_extent = gpd.read_file(\n",
    "    'Final-Project/Data/utah-avalanche-clip/utah-avalanche-clip.shp')\n",
    "\n",
    "avalanche_extent_reproj = avalanche_extent.to_crs({'init': 'epsg:26912'})\n",
    "\n",
    "# Open avalanche shapepaths\n",
    "avalanche_paths = gpd.read_file(\n",
    "    'original/vector/modified/Cottonwood_UT_paths_intersection/avalanche_intersection.shp')\n",
    "\n",
    "avalanche_paths_reproj = avalanche_paths.to_crs({'init': 'epsg:26912'})\n",
    "\n",
    "avalanche_clip = cl.clip_shp(avalanche_paths_reproj, avalanche_extent_reproj)\n",
    "avalanche_clip_reproj = avalanche_clip.to_crs({'init': 'epsg:26912'})\n",
    "\n",
    "# create crop bounds\n",
    "crop_bounds = avalanche_extent_reproj.total_bounds\n",
    "\n",
    "# create geojson object from the shapefile imported above\n",
    "extent_geojson = mapping(avalanche_extent_reproj['geometry'][0])\n",
    "\n",
    "# write geojson object to verify its cropping region\n",
    "extent_geojson\n",
    "\n",
    "# create spatial extent from the clipping layer\n",
    "cropped_spatial_extent = [crop_bounds[0],\n",
    "                          crop_bounds[2], crop_bounds[1], crop_bounds[3]]\n",
    "\n",
    "# write spatial extent to make sure we're on the right page\n",
    "cropped_spatial_extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge asc file lists to create intermediate geotiff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define individual asc tiles needed to stitch together for intermediate tif files.\n",
    "\n",
    "ftp_file_list_1 = ['12TVK360920',\n",
    "                   '12TVK360940',\n",
    "                   '12TVK380920',\n",
    "                   '12TVK380940',\n",
    "                   '12TVK400920',\n",
    "                   '12TVK400940',\n",
    "                   '12TVK420920',\n",
    "                   '12TVK420940',\n",
    "                   '12TVK440920',\n",
    "                   '12TVK440940',\n",
    "                   '12TVK460920',\n",
    "                   '12TVK460940',\n",
    "                   '12TVK480920',\n",
    "                   '12TVK480940',\n",
    "                   '12TVK500920',\n",
    "                   '12TVK500940']\n",
    "\n",
    "ftp_file_list_2 = ['12TVK360900',\n",
    "                   '12TVK380900',\n",
    "                   '12TVK400900',\n",
    "                   '12TVK420900',\n",
    "                   '12TVK440900',\n",
    "                   '12TVK460900']\n",
    "\n",
    "\n",
    "ftp_file_list_3 = ['12TVK360880',\n",
    "                   '12TVK380880',\n",
    "                   '12TVK400880',\n",
    "                   '12TVK420880']\n",
    "\n",
    "ftp_file_list_4 = ['12TVK360860',\n",
    "                   '12TVK380860',\n",
    "                   '12TVK400860']\n",
    "\n",
    "ftp_file_list_5 = ['12TVK360840',\n",
    "                   '12TVK380840']\n",
    "\n",
    "ftp_file_list_6 = ['12TVK400840']\n",
    "\n",
    "ftp_file_list_7 = ['12TVK420860']\n",
    "\n",
    "ftp_file_list_8 = ['12TVK440880',\n",
    "                   '12TVK460880']\n",
    "\n",
    "ftp_file_list_9 = ['12TVK480900',\n",
    "                   '12TVK500900']\n",
    "\n",
    "ftp_file_list_10 = ['12TVK520920',\n",
    "                    '12TVK520940']\n",
    "\n",
    "file_lists = [ftp_file_list_1, ftp_file_list_2, ftp_file_list_3, ftp_file_list_4, ftp_file_list_5,\n",
    "              ftp_file_list_6, ftp_file_list_7, ftp_file_list_8, ftp_file_list_9, ftp_file_list_10]\n",
    "\n",
    "# Create intermediate geotiff files\n",
    "for file_list in file_lists:\n",
    "    asc_to_merged_tiff(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge intermediate geotiff files to create final DEM plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find intermediate geotiff files\n",
    "avalanche_arr_list = glob(os.path.join(modified_data_dir, '*0.tif'))\n",
    "\n",
    "nfile_dem_list = []\n",
    "\n",
    "for file in avalanche_arr_list:\n",
    "    target_file = os.path.join(modified_data_dir, file)\n",
    "    nfile_dem = rio.open(target_file, no_data=-9999.0)\n",
    "    nfile_dem_list += [nfile_dem]\n",
    "\n",
    "# Merge intermediate files\n",
    "avalanche_tif_merged, out_trans = merge(\n",
    "    nfile_dem_list, res=(2.0, 2.0), nodata=-9999.0)\n",
    "\n",
    "# Write to final image to new array\n",
    "with rio.open(os.path.join(modified_data_dir, \"Final.tif\"), 'w',\n",
    "              driver=\"GTiff\", height=avalanche_tif_merged[0].shape[0], width=avalanche_tif_merged[0].shape[1],\n",
    "              count=1, dtype=str(avalanche_tif_merged[0].dtype),\n",
    "              # proj4 definition for NAD83 UTM zone 12N\n",
    "              crs=\"+proj=utm +zone=12 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\",\n",
    "              transform=out_trans,\n",
    "              nodata=-9999.0) as dst:\n",
    "\n",
    "    dst.write(avalanche_tif_merged[0], 1)\n",
    "\n",
    "# Crop and mask final image\n",
    "with rio.open(os.path.join(modified_data_dir, \"Final.tif\")) as src:\n",
    "    image = src.read(1, masked=True)\n",
    "    pre_extent = plotting_extent(src)\n",
    "    transform = src.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEM_Plot(output_file):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    \n",
    "    with rio.open(output_file) as src:\n",
    "        image = src.read(1, masked=True)\n",
    "        pre_extent = plotting_extent(src)\n",
    "        transform = src.transform\n",
    "\n",
    "    # Create DEM profile image\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    demplot = ax.imshow(image,\n",
    "                        extent=cropped_spatial_extent,\n",
    "                        cmap='viridis')\n",
    "    fig.colorbar(demplot, fraction=.04, anchor=(0.2, 0.3))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\n",
    "        \"DEM Figure 1: Little Cottonwood Canyon, UT\\nDigital Elevation Model - November 2008\", fontsize=20)\n",
    "    fig.text(.5, .2, \"Source: Utah Automated Geographic Reference Center (AGRC)\", ha='center')\n",
    "    avalanche_extent_reproj.boundary.plot(ax=ax, color='black')\n",
    "    avalanche_clip_reproj.boundary.plot(ax=ax, color='blue', linewidth=1)\n",
    "    plt.show()\n",
    "    \n",
    "    return image, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Slope_Plot(output_file):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    dem_raw = rd.LoadGDAL(output_file, no_data=-9999.0)\n",
    "\n",
    "    # Process slope and aspect using richdem\n",
    "    slope = rd.TerrainAttribute(dem_raw, attrib='slope_degrees')\n",
    "    \n",
    "    # Mask array\n",
    "    slope_ma = np.ma.masked_where(slope == dem_raw.no_data,\n",
    "                              slope,\n",
    "                              copy=True)\n",
    "    \n",
    "    # Plot masked slope array\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    slopeplot = ax.imshow(slope_ma,\n",
    "                          extent=cropped_spatial_extent,\n",
    "                          cmap='PiYG')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(slopeplot, fraction=.04, cax=cax)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"DEM Figure 2: Derived slope angles from DEM data\\nNovember 2008\", fontsize=20)\n",
    "    fig.text(.5, .2, \"Source: Utah Automated Geographic Reference Center (AGRC)\", ha='center')\n",
    "    avalanche_extent_reproj.boundary.plot(ax=ax, color='black')\n",
    "    avalanche_clip_reproj.boundary.plot(ax=ax, color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    return slope, slope_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aspect_Plot(output_file):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    dem_raw = rd.LoadGDAL(output_file, no_data=-9999.0)\n",
    "\n",
    "    # Process slope and aspect using richdem\n",
    "    dem_aspect = rd.TerrainAttribute(dem_raw, attrib='aspect')\n",
    "    \n",
    "    # Mask array\n",
    "    aspect_ma = np.ma.masked_where(dem_aspect == dem_raw.no_data,\n",
    "                                   dem_aspect,\n",
    "                                   copy=True)\n",
    "    \n",
    "    # Plot aspect profile\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    hillshade_plot = ax.imshow(aspect_ma,\n",
    "                               extent=cropped_spatial_extent,\n",
    "                               cmap='twilight_shifted')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(hillshade_plot, fraction=.04, cax=cax)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"DEM Figure 3: Derived Aspect from DEM Data\\nNovember 2008\", fontsize=20)\n",
    "    fig.text(.5, .2, \"Source: Utah Automated Geographic Reference Center (AGRC)\", ha='center')\n",
    "    avalanche_extent_reproj.boundary.plot(ax=ax, color='black')\n",
    "    avalanche_clip_reproj.boundary.plot(ax=ax, color='chartreuse')\n",
    "    plt.show()\n",
    "    \n",
    "    return dem_aspect, aspect_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Danger_Plot(slope_ma, aspect_ma):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    # Define slope and aspect classification bins\n",
    "    slope_ma[slope_ma <= 25] = 1\n",
    "    slope_ma[(slope_ma > 25) & (slope_ma <= 35)] = 2\n",
    "    slope_ma[(slope_ma > 35) & (slope_ma <= 45)] = 3\n",
    "    slope_ma[(slope_ma > 45) & (slope_ma <= 55)] = 2\n",
    "    slope_ma[slope_ma > 55] = 1\n",
    "\n",
    "    aspect_ma[aspect_ma <= 45] = 3\n",
    "    aspect_ma[(aspect_ma > 45) & (aspect_ma <= 225)] = 2\n",
    "    aspect_ma[(aspect_ma > 225) & (aspect_ma <= 315)] = 1\n",
    "    aspect_ma[aspect_ma > 315] = 3\n",
    "\n",
    "    avy_danger_score = slope_ma*0.75 + aspect_ma*0.25\n",
    "\n",
    "    # Define avalanche danger thresholds\n",
    "    avy_danger_bins = [-np.inf, 1, 1.5, 2, 2.5, np.inf]\n",
    "    \n",
    "    avy_danger_class = np.digitize(avy_danger_score, avy_danger_bins, right=True)\n",
    "    \n",
    "    # Create color dictionary for danger score\n",
    "    nbr_colors = [\"g\", \"yellowgreen\", \"peachpuff\", \"coral\", \"maroon\"]\n",
    "    nbr_cmap = ListedColormap(nbr_colors)\n",
    "    \n",
    "    # Avy site data prep to plot over danger score\n",
    "    data_fp = os.path.join('original', 'avi-stats')\n",
    "    input_csv_fn = glob(os.path.join(data_fp + '/site*.csv'))\n",
    "    avalanche_2011_df = pd.read_csv(input_csv_fn[0])\n",
    "    avalanche_2014_df = pd.read_csv(input_csv_fn[1])\n",
    "    avalanche_2017_df = pd.read_csv(input_csv_fn[2])\n",
    "\n",
    "    # Put the latitude and longitude for each year from the dataframe into numpy array\n",
    "    lats_lons_2011 = avalanche_2011_df.iloc[:, 7:9].values\n",
    "    lats_lons_2014 = avalanche_2014_df.iloc[:, 7:9].values\n",
    "    lats_lons_2017 = avalanche_2017_df.iloc[:, 7:9].values\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "    im = ax.imshow(avy_danger_class,\n",
    "                   extent=cropped_spatial_extent,\n",
    "                   cmap=nbr_cmap)\n",
    "\n",
    "    avalanche_extent_reproj.boundary.plot(ax=ax,\n",
    "                                          edgecolor='black',\n",
    "                                          linewidth=2)\n",
    "\n",
    "    avalanche_clip_reproj.boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "\n",
    "    # Create legend\n",
    "    values = np.unique(avy_danger_class.ravel())\n",
    "    avy_cat_names = [\"No Danger\",\n",
    "                     \"Low Danger\",\n",
    "                     \"Moderate Danger\",\n",
    "                     \"High Danger\",\n",
    "                     \"Extreme Danger\"]\n",
    "    es.draw_legend(im,\n",
    "                   classes=values,\n",
    "                   titles=avy_cat_names,\n",
    "                   bbox=(0.7, 0.25))\n",
    "\n",
    "    # Loop through each 2011 lat/lon, convert to geopandas dataframe, and plot\n",
    "    for coord_2011 in lats_lons_2011:\n",
    "        avi_site = np.array([[coord_2011[1], coord_2011[0]]]\n",
    "                            )  # longtitude, latitude\n",
    "        geometry = [Point(xy) for xy in avi_site]\n",
    "        point_loc = gpd.GeoDataFrame(geometry,\n",
    "                                     columns=['geometry'],\n",
    "                                     crs={'init': 'epsg:4326'})\n",
    "        point_loc.to_crs(epsg=26912).plot(ax=ax,\n",
    "                                          markersize=50,\n",
    "                                          color='blue')\n",
    "\n",
    "    # Loop through each 2014 lat/lon, convert to geopandas dataframe, and plot\n",
    "    for coord_2014 in lats_lons_2014:\n",
    "        avi_site = np.array([[coord_2014[1], coord_2014[0]]]\n",
    "                            )  # longtitude, latitude\n",
    "        geometry = [Point(xy) for xy in avi_site]\n",
    "        point_loc = gpd.GeoDataFrame(geometry,\n",
    "                                     columns=['geometry'],\n",
    "                                     crs={'init': 'epsg:4326'})\n",
    "        point_loc.to_crs(epsg=26912).plot(ax=ax,\n",
    "                                          markersize=50,\n",
    "                                          color='blue')\n",
    "\n",
    "    # Loop through each 2017 lat/lon, convert to geopandas dataframe, and plot\n",
    "    for coord_2017 in lats_lons_2017:\n",
    "        avi_site = np.array([[coord_2017[1], coord_2017[0]]]\n",
    "                            )  # longtitude, latitude\n",
    "        geometry = [Point(xy) for xy in avi_site]\n",
    "        point_loc = gpd.GeoDataFrame(geometry,\n",
    "                                     columns=['geometry'],\n",
    "                                     crs={'init': 'epsg:4326'})\n",
    "        point_loc.to_crs(epsg=26912).plot(ax=ax,\n",
    "                                          markersize=50,\n",
    "                                          color='blue')\n",
    "\n",
    "    blue_dot = Line2D([0], [0], marker='o', color='w', label='Avalanche site',\n",
    "                      markerfacecolor='b', markersize=10)\n",
    "    #ax.legend(handles=[blue_dot], bbox_to_anchor=(0.75, 0.1))\n",
    "\n",
    "    ax.set_title(\"Figure 4. Avalanche Danger in Little Cottonwood Canyon\\nAvalanche Sites for 2011, 2014, 2017\",\n",
    "                 fontsize=20)\n",
    "\n",
    "    fig.text(.5, .2, \"Imagery Source: Utah Automated Geographic Reference Center (AGRC)\\nAvalanche site source: Utah Avalanche Center\", ha='center')\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return avy_danger_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_Danger(avy_danger_class):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    mean_danger = get_zonal_stats_dataframe(\n",
    "        avalanche_clip_reproj, avy_danger_class, transform, 'mean')\n",
    "\n",
    "    avalanche_clip_reproj_danger_mask = mean_danger[mean_danger[\"mean\"].notnull()]\n",
    "\n",
    "    avalanche_clip_reproj_danger_mask_update = avalanche_clip_reproj_danger_mask[\n",
    "        avalanche_clip_reproj_danger_mask[\"mean\"] > 1]\n",
    "    \n",
    "    return avalanche_clip_reproj_danger_mask_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalized_mean_danger(image, avy_danger):\n",
    "    '''\n",
    "    Imports a list of ascii files and merges them into a single geotiff file\n",
    "    using rasterio.merge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asc_list: list\n",
    "        The .asc files that contain the raw DEM data downloaded from Utah AGRC\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    arr: numpy array\n",
    "        The cropped landsat raw data.\n",
    "    out_trans: geotransform object\n",
    "        The transform object used to create the geotiff file\n",
    "    pre_extent: Polygon object\n",
    "        The spatial extent of the DEM data array\n",
    "    '''\n",
    "    sf = shapefile.Reader(\n",
    "        'original/vector/modified/Cottonwood_UT_paths_intersection/avalanche_intersection.shp')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "    im = ax.imshow(image,\n",
    "                   extent=cropped_spatial_extent)\n",
    "\n",
    "    avalanche_extent_reproj.boundary.plot(ax=ax,\n",
    "                                          edgecolor='black',\n",
    "                                          linewidth=2)\n",
    "\n",
    "    avalanche_clip_reproj.boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "\n",
    "    # get list of field names, pull out appropriate index\n",
    "    # fieldnames of interest: Shape_Area for us.\n",
    "    fld = sf.fields[1:]\n",
    "    field_names = [field[0] for field in fld]\n",
    "    fld_name = 'Shape_Area'\n",
    "    fld_ndx = field_names.index(fld_name)\n",
    "\n",
    "    # loop over records, track global min/max\n",
    "    maxmean = -9e20\n",
    "    minmean = 9e20\n",
    "\n",
    "    for shapeRec in sf.iterShapeRecords():\n",
    "        # pull out shape geometry and records\n",
    "        shape = shapeRec.shape\n",
    "        rec = shapeRec.record\n",
    "\n",
    "        area = rec[fld_ndx]\n",
    "        # minrec=np.min((minrec,rec[fld_ndx]))\n",
    "        color_src_2 = avy_danger[avy_danger.Shape_Area == area]\n",
    "        maxmean = np.max((maxmean, color_src_2[\"mean\"]))\n",
    "        minmean = np.min((minmean, color_src_2[\"mean\"]))\n",
    "\n",
    "        # define polygon fill color (facecolor) RGB values:\n",
    "        R = 1\n",
    "        G = (color_src_2[\"mean\"]-minmean)/(maxmean-minmean)\n",
    "        G = G * (G <= 1) + 1.0 * (G > 1.0)\n",
    "        B = 0\n",
    "\n",
    "        # check number of parts\n",
    "        nparts = len(shape.parts)  # total parts\n",
    "        if nparts == 1:\n",
    "            polygon = Polygon(shape.points)\n",
    "            patch = PolygonPatch(polygon, facecolor=[R, G, B], alpha=1.0, zorder=2)\n",
    "            ax.add_patch(patch)\n",
    "\n",
    "        else:  # loop over parts of each shape, plot separately\n",
    "            for ip in range(nparts):  # loop over parts, plot separately\n",
    "                i0 = shape.parts[ip]\n",
    "                if ip < nparts-1:\n",
    "                    i1 = shape.parts[ip+1]-1\n",
    "                else:\n",
    "                    i1 = len(shape.points)\n",
    "\n",
    "                polygon = Polygon(shape.points[i0:i1+1])\n",
    "                patch = PolygonPatch(polygon, facecolor=[\n",
    "                                     R, G, B], alpha=1.0, zorder=2)\n",
    "                ax.add_patch(patch)\n",
    "\n",
    "    N = 21\n",
    "    cmap = plt.get_cmap('YlOrRd', N)\n",
    "\n",
    "    norm = mpl.colors.Normalize(vmin=1, vmax=5)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ticks=np.linspace(1, 5, N),\n",
    "                 boundaries=np.arange(0.9, 5.1, .1), fraction=0.04)\n",
    "\n",
    "    ax.set_title(\"Normalized Mean Avalanche Danger Score\\nWithin Known Avalanche Paths\",\n",
    "                 fontsize=24)\n",
    "\n",
    "    fig.text(.5, .15, \"Source: Utah Automated Geographic Reference Center (AGRC)\", ha='center')\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earth-analytics-python]",
   "language": "python",
   "name": "conda-env-earth-analytics-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
