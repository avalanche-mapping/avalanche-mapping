{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the lat/lon coordinates from the Utah Avalanche Center's web site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import Request, urlopen\n",
    "from shapely.geometry import Point, Polygon\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import earthpy as et\n",
    "\n",
    "# set working dir\n",
    "os.chdir(os.path.join(et.io.HOME, \"earth-analytics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTML(link):\n",
    "    \"\"\"\n",
    "    Captures the HTML source code from a web site.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    link : string\n",
    "        A url to a web site\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r_text : string\n",
    "        The source code for the HTML page\n",
    "    \"\"\"\n",
    "\n",
    "    # Get HTML source code from website\n",
    "    q = Request(link)\n",
    "    q.add_header('User-Agent', 'Mozilla/5.0')\n",
    "    r = urlopen(q).read()\n",
    "    r_text = r.decode(\"utf-8\")\n",
    "    return r_text\n",
    "\n",
    "\n",
    "def getCoords(text):\n",
    "    \"\"\"\n",
    "    Parse the lat/lon coordinates from an avalanche's web page on the Utah Avalanche Center website.\n",
    "    Sets the coordinates to a missing value of -999 if none are available from the web page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        HTML source code\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coords[1] : string\n",
    "        latitude\n",
    "    coords[2] : string\n",
    "        longitude\n",
    "    \"\"\"\n",
    "\n",
    "    # Use regex to pull out lat and lon from HTML\n",
    "    m = re.search('wkt\":\"POINT \\((.+?)\\)\",\"projection', text)\n",
    "    if m is not None:\n",
    "        # Get values\n",
    "        coords = m.group(1).split()\n",
    "    else:\n",
    "        # Set a missing value\n",
    "        coords = [-999, -999]\n",
    "\n",
    "    # Return the latitude and longitude\n",
    "    return coords[1], coords[0]\n",
    "\n",
    "\n",
    "def parseHTML(html, term1, term2):\n",
    "    \"\"\"\n",
    "    Parses text that is between two search terms in HTML source code.\n",
    "    If search rerturns nothing, a missing value of -999 is set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : string\n",
    "        HTML source \n",
    "    term1 : string\n",
    "        Opening search term \n",
    "    term2 : string\n",
    "        Closing search term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : string\n",
    "        The text that is between the two search terms\n",
    "    \"\"\"\n",
    "\n",
    "    m = re.search(term1+\"(.+?)\"+term2, html)\n",
    "    if m is not None:\n",
    "        # Get value\n",
    "        text = m.group(1)\n",
    "    else:\n",
    "        # Set a missing value\n",
    "        text = \"-999\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def getAviData(url):\n",
    "    \"\"\"\n",
    "    Parses lat/lon, elevation, slope angle, vertical, and aspect avalanche stats\n",
    "    from an avalanche's web page on the Utah Avalanche Center (UAC) website.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        URL from the UAC website of an avalanche occurrence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_list : \n",
    "        Contains lat, lon, elevation, slope_angle, vertical, and aspect in that order\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the HTML source code form the url\n",
    "    source_code_text = getHTML(url)\n",
    "\n",
    "    # Parse lat/lon\n",
    "    lat, lon = getCoords(source_code_text)\n",
    "\n",
    "    # Parse elevation\n",
    "    search_term_1 = 'Elevation</div>\\n        <div class=\"text_02 mb2\">'\n",
    "    search_term_2 = '</div>'\n",
    "    elevation = parseHTML(source_code_text, search_term_1, search_term_2)\n",
    "\n",
    "    # Parse slope angle\n",
    "    search_term_3 = 'Slope Angle</div>\\n        <div class=\"text_02 mb2\">'\n",
    "    search_term_4 = '</div>'\n",
    "    slope_angle = parseHTML(source_code_text, search_term_3, search_term_4)\n",
    "\n",
    "    # Parse vertical\n",
    "    search_term_5 = 'Vertical</div>\\n        <div class=\"text_02 mb2\">'\n",
    "    search_term_6 = '</div>'\n",
    "    vertical = parseHTML(source_code_text, search_term_5, search_term_6)\n",
    "\n",
    "    # Parse aspect\n",
    "    search_term_7 = 'Aspect</div>\\n        <div class=\"text_02 mb2\">'\n",
    "    search_term_8 = '</div>'\n",
    "    aspect = parseHTML(source_code_text, search_term_7, search_term_8)\n",
    "\n",
    "    # Write data to dataframe\n",
    "    data_list = [lat, lon, elevation, slope_angle, vertical, aspect]\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def containsPoint(latitude, longitude, boundary):\n",
    "    \"\"\"\n",
    "    Determines if a point is within a shalpefile boundary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        Latitude of the point you want to check\n",
    "    longitude : float\n",
    "        Longitude of the point you want to check\n",
    "    boundary : geopandas object\n",
    "        Dataframe with one shapefile boundary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    boolean : boolean\n",
    "        Returns True if the point falls within the boundary and returns False if it does not\n",
    "    \"\"\"\n",
    "\n",
    "    # Conver the lat/lon to a geopandas dataframe\n",
    "    lat_lon_point = np.array([[longitude, latitude]])\n",
    "    geometry = [Point(xy) for xy in lat_lon_point]\n",
    "    point_loc = gpd.GeoDataFrame(geometry,\n",
    "                                 columns=['geometry'],\n",
    "                                 crs={'init': 'epsg:4326'})\n",
    "\n",
    "    # Check if the point is within the boundary\n",
    "    boolean = boundary.contains(point_loc)[0]\n",
    "\n",
    "    return boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All output files exist\n"
     ]
    }
   ],
   "source": [
    "# Create data and output file paths\n",
    "data_fp = os.path.join('data', 'final-project',\n",
    "                       'cottonwood-heights-utah', 'avalanche-data')\n",
    "output_fp = \"output\"\n",
    "\n",
    "# Get list of final output files if they exist\n",
    "final_csv_fn = glob(os.path.join(data_fp, output_fp + '/site-cleaned*.csv'))\n",
    "final_csv_fn.sort()\n",
    "\n",
    "# Get list of paths for each input csv file and sort\n",
    "input_csv_fn = glob(os.path.join(data_fp + '/*search*.csv'))\n",
    "input_csv_fn.sort()\n",
    "\n",
    "# Open site boundary and reproject to EPSG 4326\n",
    "site_boundary_path = os.path.join(\n",
    "    'data', 'final-project', 'cottonwood-heights-utah', 'vector-clip', 'utah-avalanche-clip.shp')\n",
    "avalanche_boundary = gpd.read_file(site_boundary_path)\n",
    "avalanche_boundary_4326 = avalanche_boundary.to_crs(epsg=4326)\n",
    "\n",
    "if len(input_csv_fn) == len(final_csv_fn):\n",
    "    # Nothing to do. All ouput files exist\n",
    "    print(\"All output files exist\")\n",
    "\n",
    "else:\n",
    "    # Need to process data to create some or all output files\n",
    "    # Loop through each input csv file\n",
    "    for file in input_csv_fn:\n",
    "        # Get year from the file name\n",
    "        year = file[93:97]\n",
    "\n",
    "        # Build final csv output file name that will contain data within the site boundary only\n",
    "        final_output_fn = os.path.join(\n",
    "            data_fp, output_fp, \"site-cleaned-utah-avalanche-data-\" + year + \".csv\")\n",
    "\n",
    "        # Check if final file for current year already exists, if not, create it\n",
    "        if os.path.exists(final_output_fn) == False:\n",
    "            # Open input file and get the URLs from it\n",
    "            avalanche_df = pd.read_csv(file)\n",
    "            urls_np = avalanche_df.loc[:, 'URL'].values\n",
    "\n",
    "            # Create an empty dataframe to capture all the parsed data outputs form the HTML source code\n",
    "            avi_data_df = pd.DataFrame(\n",
    "                columns=['lat', 'lon', 'elevation', 'slope_angle', 'vertical', 'aspect'])\n",
    "\n",
    "            # Loop through each url and parse out the lat/lon, elevation, slope angle, vertical, and aspect\n",
    "            # from the HTML source code\n",
    "            for url in urls_np:\n",
    "                avi_data_list = getAviData(url)\n",
    "\n",
    "                # Create dataframe from list\n",
    "                avi_data_df.loc[len(avi_data_df)] = avi_data_list\n",
    "\n",
    "            # Concatenate the avalanche_df with the avi_data_df\n",
    "            all_avi_data_df = pd.concat([avalanche_df, avi_data_df], axis=1)\n",
    "\n",
    "            all_avi_data_np = all_avi_data_df[['Date', 'Specific Region', 'General Region', 'Trigger',\n",
    "                                               'Depth (ft)', 'Width (ft)', 'URL', 'lat', 'lon', 'elevation',\n",
    "                                               'slope_angle', 'vertical', 'aspect']].values\n",
    "\n",
    "            # Loop through each lat/lon pair and see if it falls within the site boundary\n",
    "            avi_in_site_list = []\n",
    "            for avi_info in all_avi_data_np:\n",
    "                boolean = containsPoint(float(avi_info[7]), float(\n",
    "                    avi_info[8]), avalanche_boundary_4326)\n",
    "                if boolean:\n",
    "                    avi_in_site_list.append([avi_info[0], avi_info[1], avi_info[2], avi_info[3], avi_info[4],\n",
    "                                             avi_info[5], avi_info[6], avi_info[7], avi_info[8], avi_info[9],\n",
    "                                             avi_info[10], avi_info[11], avi_info[12]])\n",
    "\n",
    "            # Create pandas dataframe from list\n",
    "            labels = ['date', 'specific_region', 'general_region', 'trigger', 'depth_ft',\n",
    "                      'width_ft', 'url', 'lat', 'lon', 'elevation_ft', 'slope_angle_deg', 'vertical_ft', 'aspect']\n",
    "            avi_in_site_df = pd.DataFrame.from_records(\n",
    "                avi_in_site_list, columns=labels)\n",
    "\n",
    "            # Write the avalanche data that is within the site to a csv file\n",
    "            avi_in_site_df.to_csv(final_output_fn, index=False)\n",
    "        else:\n",
    "            print(\"Final data for \" + year +\n",
    "                  \" alreday exists. No processing needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earth-analytics-python]",
   "language": "python",
   "name": "conda-env-earth-analytics-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
